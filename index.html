<!DOCTYPE html>

<html>

<head>
    <style>
        td,
        th {
            border: 0px solid black;
        }

        img {
            padding: 5px;
        }
    </style>

    <title>FaceXFormer</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="shortcut icon" href="./static/images/jhu_web.png" />

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">
    <link rel="stylesheet" href="https://unpkg.com/image-compare-viewer/dist/image-compare-viewer.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://unpkg.com/image-compare-viewer/dist/image-compare-viewer.min.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

</head>

<body>


    <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-2 publication-title"> <i>FaceXFormer</i> : A Unified Transformer <br> for
                            Facial
                            Analysis
                        </h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Group of first four authors -->
                            <div class="authors-group">
                                <span class="author-block">
                                    <a href="https://kartik-3004.github.io/portfolio/" target="_blank">Kartik
                                        Narayan*</a>,
                                </span>
                                <span class="author-block">
                                    <a href="https://vibashan.github.io/" target="_blank">Vibashan VS*</a>,
                                </span>
                                <span class="author-block">
                                    <a href="https://scholar.google.com/citations?user=L60tuywAAAAJ&hl=en"
                                        target="_blank">Rama Chellappa</a>,
                                </span>
                                <span class="author-block">
                                    <a href="https://scholar.google.com/citations?user=AkEXTbIAAAAJ&hl=en"
                                        target="_blank">Vishal M. Patel</a>
                                </span>
                            </div>
                        </div>


                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Johns Hopkins University</span>
                        </div>

                        <div class="column has-text-centered">
                            <a href="as"></a>
                            </span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <!-- PDF Link. -->
                                <!-- <span class="link-block">
                                    <a href=""
                                        class="external-link button is-normal is-rounded is-dark" target="_blank">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span> -->
                                <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary material</span>
                </a>
              </span> -->
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>Arxiv</span>
                                    </a>
                                </span>
                                <span class="link-block">
                                    <a href="" class="external-link button is-normal is-rounded is-dark">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                            </div>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="section">
        <div class="container is-max-desktop">
            <!-- Abstract. -->
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">

                    <h2 class="title is-3">Motivation & Contribution</h2>
                    <img src="./static/images/intro_viz.png" alt="" border="0" height="600" width="1500">
                    <img src="./static/images/intro.png" alt="" style="border:0; height:500px; width:1500px;">
                    <div class="content has-text-justified">
                        <p>Comparison with representative methods under different task settings.
                            <i>FaceXformer</i>
                            can perform various facial analysis tasks in single model. FP
                            - Face Parsing, LD - Landmarks Detection, HPE - Head Pose Estimation, Attr - At-
                            tributes Recognition, Age - Age Estimatin, Gen - Gender Estimation, Race - Race
                            Estimation, Vis - Landmarks Visibility Prediction, MD - Multi-dataset Training
                        </p>
                        <ul>
                            <li>
                                In recent years, significant advancements have been made in facial analysis, developing
                                state-of-the-art methods for
                                various tasks. Despite these methods achieving promising performance, they cannot be
                                integrated into a single pipeline due to their
                                specialized model designs and task-specific pre-processing techniques.
                            </li>
                            <li> <i>FaceXformer</i> is an end-to-end unified model capable of handling a comprehensive
                                range of facial analysis tasks such as face parsing, landmark detection, head pose
                                estimation, attributes recognition, and estimation of age, gender, race, and landmarks
                                visibility.
                            </li>
                            <li> It leverages a transformer-based encoder-decoder architecture where
                                each task is treated as a learnable token, enabling the integration of multiple tasks
                                within a single framework.
                            </li>
                            <li> It effectively handles images "in-the-wild," demonstrating its robustness and
                                generalizability across eight heterogenous tasks, all while maintaining the real-time
                                performance of 37 FPS.</li>
                        </ul>

                        </p>
                    </div>
                </div>
            </div>
            <!--/ Abstract. -->

            <!-- Paper video. -->

            <section class="section">
                <div class="container is-max-desktop">
                    <!-- Abstract. -->
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <h2 class="title is-3"><i>FaceXformer</i> Framework</h2>
                            <div class="content has-text-justified">
                                <h5 class="subtitle has-text-centered"></h5>

                                <img src="./static/images/main_archi.png" alt="" border=0 height=500 width=1500></img></
                                    <p>
                                Overview of <i>FaceXformer</i> framework. It employs an encoder-decoder
                                architecture, extracting multi-scale features from the input face image <b>I</b>, and
                                fusing them into a unified representation <b>F</b> via MLP-Fusion. Task tokens <b>T</b>
                                are processed alongside face representation <b>F</b> in the decoder, resulting in
                                refined
                                task-specific tokens <b><span style="position: relative; display: inline-block;">
                                        T
                                        <span
                                            style="position: absolute; top: -7px; left: 0.6px; right: 0; font-size: smaller;">^</span>
                                    </span></b>. These refined tokens are then used for
                                task-specific predictions by passing through the unified head.
                                </p>


                            </div>
                        </div>
                    </div>
                </div>
            </section>

            <section class="section">
                <div class="container is-max-desktop">
                    <!-- Abstract. -->
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <h2 class="title is-3">Quantitative Results</h2>
                            <div class="content has-text-justified">
                                <h5 class="subtitle has-text-centered"></h5>
                                <img src="./static/images/quantitative_results.png" alt="" border=0 height=500
                                    width=1500></img></ </div>
                                <p>
                                    Comparison with specialized models and existing multi-task networks.
                                </p>
                            </div>
                        </div>
                    </div>
            </section>

            <section class="section">
                <div class="container is-max-desktop">
                    <!-- Abstract. -->
                    <div class="columns is-centered has-text-centered">
                        <div class="column is-four-fifths">
                            <h2 class="title is-3">Qualitative Results</h2>
                            <div class="content has-text-justified">
                                <h5 class="subtitle has-text-centered"></h5>
                                <img src="./static/images/viz_qual_result.png" alt="" border=0 height=500
                                    width=1500></img></ <p>
                                Qualitative comparison of <i>FaceXformer</i> against other multi-task models
                                </p>
                            </div>
                            <div class="content has-text-justified">
                                <h5 class="subtitle has-text-centered"></h5>
                                <img src="./static/images/viz_inthewild.png" alt="" border=0 height=500
                                    width=1500></img></ <p>
                                Visualization of "in-the-wild" images queried for multiple task tokens. Attributes
                                represent the 40 binary attributes defined in the CelebA dataset, indicating the
                                presence (1) or absence (0) of specific facial attributes
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
            </section>


            <section class="section" id="BibTeX">
                <div class="container content is-max-desktop">
                    <h2 class="title">BibTeX</h2>
                    <pre><code>
Coming soon ...
  <!-- @InProceedings{Narayan_2023_CVPR,
    author    = {Narayan, Kartik and Agarwal, Harsh and Thakral, Kartik and Mittal, Surbhi and Vatsa, Mayank and Singh, Richa},
    title     = {DF-Platter: Multi-Face Heterogeneous Deepfake Dataset},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2023},
    pages     = {9739-9748}
} -->
</code></pre>
                </div>
            </section>

            <section class="section">
                <div class="container is-max-desktop content">
                    <h5 class="title" style="font-size: 10px;"> Acknowledgement: The website template is taken from
                        <span class="author-block">
                            <a href="https://nerfies.github.io/" target="_blank">Nerfies</a>
                    </h5>

                </div>
            </section>

            <script>
                const viewers = document.querySelectorAll(".image-compare");
                viewers.forEach((element) => {
                    let view = new ImageCompare(element, {
                        hoverStart: true,
                        addCircle: true
                    }).mount();
                });

                $(document).ready(function () {
                    var editor = CodeMirror.fromTextArea(document.getElementById("bibtex"), {
                        lineNumbers: false,
                        lineWrapping: true,
                        readOnly: true
                    });
                    $(function () {
                        $('[data-toggle="tooltip"]').tooltip()
                    })
                });
            </script>
</body>

</html>